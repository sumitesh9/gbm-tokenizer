import sentencepiece as spm
import os
import json
from collections import Counter

# Try to import optional dependencies
try:
    import tiktoken
    TIKTOKEN_AVAILABLE = True
except ImportError:
    TIKTOKEN_AVAILABLE = False

try:
    from transformers import AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

class TokenizerWrapper:
    """Wrapper class to provide unified interface for different tokenizers."""
    def __init__(self, name, tokenizer, vocab_size=None, tokenizer_type="sentencepiece"):
        self.name = name
        self.tokenizer = tokenizer
        self._vocab_size = vocab_size
        self.tokenizer_type = tokenizer_type
    
    def encode(self, text, out_type=str):
        """Encode text to tokens."""
        if self.tokenizer_type == "sentencepiece":
            return self.tokenizer.encode(text, out_type=out_type)
        elif self.tokenizer_type == "tiktoken":
            ids = self.tokenizer.encode(text, allowed_special="all")
            if out_type is str:
                # For tiktoken, return byte string representations
                # Each token is decoded individually to show what it represents
                return [self.tokenizer.decode([id]) for id in ids]
            else:
                return ids
        elif self.tokenizer_type == "transformers":
            if out_type is str:
                return self.tokenizer.tokenize(text)
            else:
                return self.tokenizer.encode(text, add_special_tokens=False)
        elif self.tokenizer_type == "character":
            if out_type is str:
                return list(text)
            else:
                return [ord(c) for c in text]
        else:
            raise ValueError(f"Unknown tokenizer type: {self.tokenizer_type}")
    
    def decode(self, tokens):
        """Decode tokens back to text."""
        if not tokens:
            return ""
        
        if self.tokenizer_type == "sentencepiece":
            return self.tokenizer.decode(tokens)
        elif self.tokenizer_type == "tiktoken":
            if isinstance(tokens[0], str):
                # For tiktoken, string tokens are actually byte representations
                # We need to reconstruct from the original encoding
                # This is a limitation - tiktoken works best with IDs
                # Try to encode each token string and get IDs
                ids = []
                for token_str in tokens:
                    try:
                        # Encode the token string to get its ID
                        token_ids = self.tokenizer.encode(token_str, allowed_special="all")
                        if token_ids:
                            ids.extend(token_ids)
                    except Exception:
                        pass
                return self.tokenizer.decode(ids) if ids else ''.join(tokens)
            else:
                return self.tokenizer.decode(tokens)
        elif self.tokenizer_type == "transformers":
            if isinstance(tokens[0], str):
                return self.tokenizer.convert_tokens_to_string(tokens)
            else:
                return self.tokenizer.decode(tokens, skip_special_tokens=True)
        elif self.tokenizer_type == "character":
            if isinstance(tokens[0], str):
                return ''.join(tokens)
            else:
                return ''.join(chr(t) if t < 0x110000 else '?' for t in tokens)
        else:
            raise ValueError(f"Unknown tokenizer type: {self.tokenizer_type}")
    
    def get_vocab_size(self):
        """Get vocabulary size."""
        if self._vocab_size:
            return self._vocab_size
        if self.tokenizer_type == "sentencepiece":
            return self.tokenizer.get_piece_size()
        elif self.tokenizer_type == "tiktoken":
            return len(self.tokenizer._mergeable_ranks) if hasattr(self.tokenizer, '_mergeable_ranks') else 0
        elif self.tokenizer_type == "transformers":
            return self.tokenizer.vocab_size if hasattr(self.tokenizer, 'vocab_size') else 0
        elif self.tokenizer_type == "character":
            return 0  # Character-level has no fixed vocab
        else:
            return 0

def load_tokenizer(model_path="gbm_tokenizer.model"):
    """Load the trained tokenizer model."""
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Tokenizer model not found: {model_path}")
    
    sp = spm.SentencePieceProcessor()
    sp.load(model_path)
    return TokenizerWrapper("GBM Tokenizer (Custom)", sp, tokenizer_type="sentencepiece")

def load_comparison_tokenizers():
    """Load popular tokenizers for comparison."""
    tokenizers = []
    
    # Character-level tokenizer (baseline)
    class CharTokenizer:
        pass
    tokenizers.append(TokenizerWrapper("Character-level (Baseline)", CharTokenizer(), vocab_size=0, tokenizer_type="character"))
    
    # GPT-2 tokenizer (tiktoken)
    if TIKTOKEN_AVAILABLE:
        try:
            gpt2_tokenizer = tiktoken.get_encoding("gpt2")
            tokenizers.append(TokenizerWrapper("GPT-2 (tiktoken)", gpt2_tokenizer, vocab_size=50257, tokenizer_type="tiktoken"))
        except Exception as e:
            print(f"  Warning: Could not load GPT-2 tokenizer: {e}")
    
    # GPT-4 tokenizer (cl100k_base)
    if TIKTOKEN_AVAILABLE:
        try:
            gpt4_tokenizer = tiktoken.get_encoding("cl100k_base")
            tokenizers.append(TokenizerWrapper("GPT-4/Claude (cl100k_base)", gpt4_tokenizer, vocab_size=100256, tokenizer_type="tiktoken"))
        except Exception as e:
            print(f"  Warning: Could not load GPT-4 tokenizer: {e}")
    
    # HuggingFace tokenizers
    if TRANSFORMERS_AVAILABLE:
        try:
            bert_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
            tokenizers.append(TokenizerWrapper("BERT (bert-base-uncased)", bert_tokenizer, vocab_size=30522, tokenizer_type="transformers"))
        except Exception as e:
            print(f"  Warning: Could not load BERT tokenizer: {e}")
    
    return tokenizers

def evaluate_round_trip(tokenizer, texts):
    """Evaluate round-trip encoding/decoding accuracy."""
    correct = 0
    total = len(texts)
    
    for text in texts:
        try:
            # Use IDs for more reliable round-trip checking
            ids = tokenizer.encode(text, out_type=int)
            decoded = tokenizer.decode(ids)
            if decoded == text:
                correct += 1
        except Exception:
            pass  # Skip if encoding fails
    
    accuracy = (correct / total) * 100 if total > 0 else 0
    return accuracy, correct, total

def calculate_compression_ratio(tokenizer, texts):
    """Calculate compression ratio (characters vs tokens)."""
    total_chars = 0
    total_tokens = 0
    
    for text in texts:
        try:
            total_chars += len(text)
            tokens = tokenizer.encode(text, out_type=str)
            total_tokens += len(tokens)
        except Exception:
            pass  # Skip if encoding fails
    
    if total_tokens == 0:
        return 0, total_chars, 0
    
    ratio = total_chars / total_tokens
    return ratio, total_chars, total_tokens

def analyze_vocabulary(tokenizer):
    """Analyze the tokenizer vocabulary."""
    vocab_size = tokenizer.get_vocab_size()
    return vocab_size

def evaluate_on_corpus(corpus_path="eval.txt"):
    """Evaluate tokenizer on the evaluation test cases."""
    if not os.path.exists(corpus_path):
        print(f"Warning: Evaluation file not found: {corpus_path}")
        # Fallback to corpus.txt if eval.txt doesn't exist
        if corpus_path == "eval.txt" and os.path.exists("corpus.txt"):
            print("  Falling back to corpus.txt...")
            corpus_path = "corpus.txt"
        else:
            return None
    
    with open(corpus_path, 'r', encoding='utf-8') as f:
        # Skip comment lines (starting with #) and empty lines
        lines = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
    
    if not lines:
        print("Warning: Evaluation file is empty or contains only comments")
        return None
    
    return lines

def evaluate_tokenizer(tokenizer, texts):
    """Evaluate a single tokenizer and return metrics."""
    try:
        vocab_size = analyze_vocabulary(tokenizer)
        accuracy, correct, total = evaluate_round_trip(tokenizer, texts)
        ratio, total_chars, total_tokens = calculate_compression_ratio(tokenizer, texts)
        
        # Calculate average tokens per text
        token_counts = []
        for text in texts:
            try:
                tokens = tokenizer.encode(text, out_type=str)
                token_counts.append(len(tokens))
            except Exception:
                pass
        
        avg_tokens = sum(token_counts) / len(token_counts) if token_counts else 0
        
        return {
            'name': tokenizer.name,
            'vocab_size': vocab_size,
            'accuracy': accuracy,
            'compression_ratio': ratio,
            'total_tokens': total_tokens,
            'avg_tokens_per_text': avg_tokens,
            'success': True
        }
    except Exception as e:
        return {
            'name': tokenizer.name,
            'error': str(e),
            'success': False
        }

def print_statistics(tokenizer, texts):
    """Print comprehensive statistics about the tokenizer."""
    print("=" * 60)
    print("TOKENIZER EVALUATION REPORT")
    print("=" * 60)
    
    # Vocabulary analysis
    vocab_size = analyze_vocabulary(tokenizer)
    print("\nüìö Vocabulary Statistics:")
    print(f"  - Vocabulary size: {vocab_size:,}")
    print("  - Model file: gbm_tokenizer.model")
    
    # Round-trip accuracy
    accuracy, correct, total = evaluate_round_trip(tokenizer, texts)
    print("\n‚úÖ Round-trip Accuracy:")
    print(f"  - Accuracy: {accuracy:.2f}% ({correct}/{total})")
    
    # Compression analysis
    ratio, total_chars, total_tokens = calculate_compression_ratio(tokenizer, texts)
    print("\nüìä Compression Statistics:")
    print(f"  - Total characters: {total_chars:,}")
    print(f"  - Total tokens: {total_tokens:,}")
    print(f"  - Average chars per token: {ratio:.2f}")
    print(f"  - Compression ratio: {ratio:.2f}x")
    
    # Token distribution
    all_tokens = []
    token_lengths = []
    for text in texts:
        try:
            tokens = tokenizer.encode(text, out_type=str)
            all_tokens.extend(tokens)
            token_lengths.extend([len(token) for token in tokens])
        except Exception:
            pass
    
    if token_lengths:
        avg_token_len = sum(token_lengths) / len(token_lengths)
        max_token_len = max(token_lengths)
        min_token_len = min(token_lengths)
        
        print("\nüî§ Token Length Statistics:")
        print(f"  - Average token length: {avg_token_len:.2f} characters")
        print(f"  - Min token length: {min_token_len}")
        print(f"  - Max token length: {max_token_len}")
    
    # Most common tokens
    if all_tokens:
        token_counts = Counter(all_tokens)
        print("\nüèÜ Top 10 Most Common Tokens:")
        for token, count in token_counts.most_common(10):
            display_token = repr(token) if len(token) > 20 else token
            print(f"  - {display_token}: {count:,} occurrences")
    
    # Sample encoding
    print("\nüìù Sample Encoding (first 3 lines):")
    for i, text in enumerate(texts[:3], 1):
        try:
            tokens = tokenizer.encode(text, out_type=str)
            ids = tokenizer.encode(text, out_type=int)
            decoded = tokenizer.decode(tokens)
            
            print(f"\n  Sample {i}:")
            print(f"    Original: {text[:80]}{'...' if len(text) > 80 else ''}")
            print(f"    Tokens ({len(tokens)}): {tokens[:10]}{'...' if len(tokens) > 10 else ''}")
            print(f"    IDs ({len(ids)}): {ids[:10]}{'...' if len(ids) > 10 else ''}")
            print(f"    Decoded: {decoded[:80]}{'...' if len(decoded) > 80 else ''}")
            print(f"    Match: {'‚úì' if decoded == text else '‚úó'}")
        except Exception as e:
            print(f"  Sample {i}: Error encoding - {e}")
    
    print("\n" + "=" * 60)

def print_comparison(results):
    """Print comparison table of all tokenizers."""
    print("\n" + "=" * 80)
    print("TOKENIZER COMPARISON")
    print("=" * 80)
    
    # Filter successful results
    successful = [r for r in results if r.get('success', False)]
    
    if not successful:
        print("No tokenizers could be evaluated successfully.")
        return
    
    # Sort by compression ratio (higher is better)
    successful.sort(key=lambda x: x.get('compression_ratio', 0), reverse=True)
    
    # Print header
    print(f"\n{'Tokenizer':<30} {'Vocab Size':<12} {'Compression':<12} {'Accuracy':<10} {'Avg Tokens':<12}")
    print("-" * 80)
    
    # Print each tokenizer
    for result in successful:
        name = result['name']
        vocab = f"{result['vocab_size']:,}" if result['vocab_size'] > 0 else "N/A"
        comp = f"{result['compression_ratio']:.2f}x"
        acc = f"{result['accuracy']:.1f}%"
        avg = f"{result['avg_tokens_per_text']:.1f}"
        
        # Highlight our tokenizer
        marker = " ‚≠ê" if "GBM" in name else ""
        print(f"{name:<30}{vocab:<12}{comp:<12}{acc:<10}{avg:<12}{marker}")
    
    # Print failed tokenizers
    failed = [r for r in results if not r.get('success', False)]
    if failed:
        print("\n‚ö†Ô∏è  Failed to evaluate:")
        for result in failed:
            print(f"  - {result['name']}: {result.get('error', 'Unknown error')}")
    
    # Find best performers
    if successful:
        best_compression = max(successful, key=lambda x: x.get('compression_ratio', 0))
        best_accuracy = max(successful, key=lambda x: x.get('accuracy', 0))
        
        print("\nüèÜ Best Performance:")
        print(f"  - Best Compression: {best_compression['name']} ({best_compression['compression_ratio']:.2f}x)")
        print(f"  - Best Accuracy: {best_accuracy['name']} ({best_accuracy['accuracy']:.1f}%)")
    
    print("\n" + "=" * 80)

def main():
    """Main evaluation function."""
    print("Loading GBM tokenizer...")
    try:
        gbm_tokenizer = load_tokenizer()
        print("‚úì GBM Tokenizer loaded successfully")
    except FileNotFoundError as e:
        print(f"‚úó Error: {e}")
        print("Please train the tokenizer first using: make train")
        return
    
    print("\nLoading evaluation test cases...")
    texts = evaluate_on_corpus()
    
    if texts is None:
        print("Using sample text for evaluation...")
        texts = ["‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞ ‡§Æ‡•á‡§∞‡•Å ‡§®‡§æ‡§Æ ‡§∏‡•Å‡§Æ‡§ø‡§§‡•á‡§∂ ‡§ö"]
    
    print(f"‚úì Loaded {len(texts)} test cases for evaluation\n")
    
    # Run evaluation on GBM tokenizer
    print_statistics(gbm_tokenizer, texts)
    
    # Load and compare with other tokenizers
    print("\n" + "=" * 80)
    print("Loading comparison tokenizers...")
    print("=" * 80)
    comparison_tokenizers = load_comparison_tokenizers()
    print(f"‚úì Loaded {len(comparison_tokenizers)} comparison tokenizers")
    
    # Evaluate all tokenizers
    print("\nEvaluating all tokenizers...")
    results = []
    
    # Evaluate GBM tokenizer
    gbm_result = evaluate_tokenizer(gbm_tokenizer, texts)
    results.append(gbm_result)
    
    # Evaluate comparison tokenizers
    for tokenizer in comparison_tokenizers:
        print(f"  Evaluating {tokenizer.name}...")
        result = evaluate_tokenizer(tokenizer, texts)
        results.append(result)
    
    # Print comparison
    print_comparison(results)
    
    # Save results to JSON for chart generation
    try:
        with open('eval_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        print("\nüíæ Results saved to eval_results.json (use generate_chart.py to create charts)")
    except Exception as e:
        print(f"\n‚ö†Ô∏è  Could not save results to JSON: {e}")
    
    print("\n‚úì Evaluation complete!")

if __name__ == "__main__":
    main()
